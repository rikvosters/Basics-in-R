library(stopwords)
stoplist <- stopwords(language = 'en', source = "stopwords-iso")
corpus <- corpus[!corpus %in% stoplist]
ref.corpus <- ref.corpus[!ref.corpus %in% stoplist]
# remove empty elements
corpus <- corpus[corpus!=""]
ref.corpus <- ref.corpus[ref.corpus!=""]
# DOUBLE FREQ TABLE
# make a vector with all words from both corpora
words <- c(corpus, ref.corpus)
# make a vector with the names of each corpus, as often as each corpus contains words
corpora <- c(rep("Corpus", length(corpus)), rep("Referencecorpus", length(ref.corpus)))
# 'double' frequency table with freq of each word per corpus
freq.table <- table(words, corpora)
# save as dataframe
result <- as.data.frame.matrix(freq.table)
# explore
tail(result, 20)
### CHI SQUARE TESTS
# package 'corpora' (Evert 2015)
# install.packages("corpora")
library(corpora)
# function chisq (Evert 2004: 82):
# "The standard test for independence of the rows and columns in a contingency table, at least in the field of mathematical statistics, is Pearson’s chi-squared test […]. Its test statistic is often denoted by the symbol X2 (cf. Pedersen 1996), and is a two-sided association measure."
# chisq(k1, n1, k2, n2, correct = TRUE, one.sided=FALSE)
# k1 = frequency of a type in the first corpus
# n1 = sample size of the first corpus
# k2 = frequency of a type in the reference corpus
# n2 = sample size of the reference corpus
# add chisq as extra column to dataframe with frequencies
result$X2 <- chisq(result$Corpus, length(result$Corpus), result$Referencecorpus, length(result$Referencecorpus))
# obtain p-values (identical to chisq(), but with chisq.pval()) and add as extra column to dataframe with frequencies
result$p <- chisq.pval(result$Corpus, length(result$Corpus), result$Referencecorpus, length(result$Referencecorpus))
# explore
tail(result, 20)
# round
result$X2 <- round(result$X2, 2)
result$p <- round(result$p, 5)
# sort dataframe based on X2-value (order())
result <- result[order(result$X2, decreasing=T),]
### RESULT
# explore
head(result, 10)
# only keep results where words appear more in corpus than in reference corpus
result <- result[result$Corpus>result$Referencecorpus,]
# explore
head(result, 10)
# limit to first ten
result <- result[1:10,]
# extract frequency values for corpus and reference corpus (in new vector: 'to_plot')
to_plot <- rbind(result$Corpus, result$Referencecorpus)
to_plot
# limit to first ten
result <- result[1:10,]
# extract frequency values for corpus and reference corpus (in new vector: 'to_plot')
to_plot <- rbind(result$Corpus, result$Referencecorpus)
to_plot
# barplot
barplot(to_plot, main="Keywords (not significant)", ylab="Absolute frequency", beside = T, names.arg=rownames(result), las=2, cex.names = 0.75, legend.text=c("Corpus", "Reference corpus"))
rm(list=ls(all=T))
# corpus: CGN-A-nl.txt
# reference corpus: CGN-A-vl.txt
corpus <- scan(file="CGN-A-nl.txt", what=character(0), quote = "")
rm(list=ls(all=T))
# corpus: CGN-A-nl.txt
# reference corpus: CGN-A-vl.txt
corpus <- scan(file="CGN-A-nl.txt", what=character(0), quote = "")
# corpus: CGN-A-nl.txt
# reference corpus: CGN-A-vl.txt
corpus <- scan(file="CGN-A-nl.txt", what=character(0), quote = "")
ref.corpus <- scan(file="CGN-A-vl.txt", what=character(0), quote = "")
# remove punctuation and capitals
punctuation.remover <- function (text) {
temp <- gsub("[,.'…\"”)(“‘’;:!?]", "", text, perl=TRUE)  #  interpunctie weghalen
gsub(" {2,}", " ", temp, perl=TRUE)                #  dubbele spaties weghalen
}
corpus <- tolower(punctuation.remover(corpus))
ref.corpus <- tolower(punctuation.remover(ref.corpus))
# remove empty elements
corpus <- corpus[corpus!=""]
ref.corpus <- ref.corpus[ref.corpus!=""]
# make a vector with all words from both corpora
words <- c(corpus, ref.corpus)
# make a vector with the names of each corpus, as often as each corpus contains words
corpora <- c(rep("Corpus", length(corpus)), rep("Referencecorpus", length(ref.corpus)))
# 'double' frequency table with freq of each word per corpus
freq.table <- table(words, corpora)
# save as dataframe
result <- as.data.frame.matrix(freq.table)
# explore
tail(result, 20)
library(corpora)
# add chisq as extra column to dataframe with frequencies
result$X2 <- chisq(result$Corpus, length(result$Corpus), result$Referencecorpus, length(result$Referencecorpus))
# obtain p-values (identical to chisq(), but with chisq.pval()) and add as extra column to dataframe with frequencies
result$p <- chisq.pval(result$Corpus, length(result$Corpus), result$Referencecorpus, length(result$Referencecorpus))
# explore
tail(result, 20)
# round
result$X2 <- round(result$X2, 2)
result$p <- round(result$p, 5)
# sort dataframe based on X2-value (order())
result <- result[order(result$X2, decreasing=T),]
# explore
head(result, 10)
# only keep results where words appear more in corpus than in reference corpus
result <- result[result$Corpus>result$Referencecorpus,]
result <- result[1:25,]
# if you want: only keep significant keywords (p <= 0.05)
result <- result[result$p<=0.05,]
# explore
head(result, 10)
# save as file
write.table(result, "output/output_keyness.txt", sep="\t", row.names=T, quote=F)
# explore
head(result, 10)
# only retain the words with 25 highest X2-values
result_sign <- result[1:25,]
result_sign
# extract frequency values for corpus and reference corpus (in new vector: 'to_plot')
to_plot <- rbind(result$Corpus, result$Referencecorpus)
to_plot
# barplot
barplot(to_plot, main="Keywords (top 25)", ylab="Absolute frequency", beside = T, names.arg=rownames(result), las=2, cex.names = 0.75, legend.text=c("Corpus", "Reference corpus"))
# install.packages("wordcloud")
library("wordcloud")
install.packages("wordcloud")
# install.packages("wordcloud")
library("wordcloud")
# load
words <- scan(file = "coleridge.txt", what = character(0), fileEncoding = "UTF-8")
words <- tolower(words)
head(words, 50)
# remove punctuation
words <- gsub("[,.'…\"”)(_“‘’;:!?]", "", words, perl=TRUE)
# stoplist
stoplist <- scan("stopwords_en.txt", what=character(0), sep="\n", fileEncoding = "UTF-8")
# stoplist
stoplist <- stopwords(language = 'en', source = "stopwords-iso")
words <- words[!words %in% stoplist]
# remove numbers (many years and page/ms. numbers)
nums <- 0:10000
nums <- as.character(nums)
words <- words[!words %in% nums]
# remove empty elements
words <- words[words!=""]
# check
head(words, 50)
# freq table
table_STC <- sort(table(words), decreasing = T)
head(table_STC, 50)
# subset of 100 most frequent
table_STC <- table_STC[1:100]
# word cloud
wordcloud(words=names(table_STC), freq=table_STC, min.freq=1, max.words=1000, random.order=FALSE, random.color=TRUE, rot.per=0.3, use.r.layout=T, colors=brewer.pal(8, "Dark2"), scale = c(3.9, 0.6))
rm(list=ls(all=TRUE))
## load custom function
source("http://www.linguistics.ucsb.edu/faculty/stgries/exact_matches.r")
corpus <- c("I don't know half of you half as well as I should like.", "And I like less than half of you half as well as you deserve.")
exact.matches("half", corpus)
# Use [[4]] to subset only the 'Lines with delimited matches':
matches <- exact.matches("half", corpus)[[4]]
matches
# export
write.table(matches, file="matches_BilboBaggins.txt", sep="\t", quote=FALSE)
# Extra options in syntax:
exact.matches("half", corpus)
# Basic syntax: search expression (regex), where/what to search
# case.sens=TRUE
# characters.around=0
# change to get characters before / after match
# (default = whole element)
# lines.around=0
# change to get lines before / after match (default=0)
# clean.up.spaces=TRUE
exact.matches("half", corpus, characters.around=10)[[4]]
# export as Excel spreadsheet
write_xlsx(matches, file="matches_BilboBaggins.xlsx")
# export as Excel spreadsheet
write_xlsx(matches, path="matches_BilboBaggins.xlsx")
# export as Excel spreadsheet
write_xlsx(matches, path ="matches_BilboBaggins.xlsx")
matches
# load corpus fragment (Old Bailey)
corpus <- scan(file="Old Bailey fragment OBC-17320114.txt", what=character(0), sep="\n")
corpus
# load exact matches
source("http://www.linguistics.ucsb.edu/faculty/stgries/exact_matches.r")
# search for all instances of the forms 'I', 'he' 'us' and 'him' in the corpus
exact.matches("\\b(I|he|us|him)\\b", corpus)
matches <- exact.matches("\\b(I|he|us|him)\\b", corpus)[[4]]
matches
# export and open as spreadsheet
write.table(matches, file="matches_OldBailey.txt", sep="\t", quote=FALSE)
# prep
rm(list=ls(all=TRUE))
install.packages("styler")
styler:::set_style_transformers()
styler:::style_selection()
styler:::style_selection()
styler:::style_selection()
styler:::style_active_file()
styler:::style_active_file()
# per word (no specification of 'sep' needed)
corpus.file.lower.words <- scan(file = "Dickens-ChristmasCarol.txt", what = character(0))
# per word (no specification of 'sep' needed)
corpus.file.lower.words <- scan(file = "https://raw.githubusercontent.com/rikvosters/Basics-in-R/master/Dickens-ChristmasCarol.txt", what = character(0))
styler:::style_selection()
# load
words <- scan(file = "https://raw.githubusercontent.com/rikvosters/Basics-in-R/master/coleridge.txt", what = character(0), fileEncoding = "UTF-8")
toupper("Data exploration and descriptive statistics ")
toupper("5. Data visualization
- QQ, hist, bar, box, line, mosaic
6. Basic parametric and non-parametric inferential statistics
- distributions: one sample, two samples (parametric and non-parametric)
- difference/independence: two samples (parametric and non-parametric)
- regression family: correlation and linear regression, logistic regression
7. Working with textual data: capita selecta
- basics: frequency lists, concordances, collocations
")
# empty workspace
rm(list=ls(all=TRUE))
tt <- read.csv(file = "Titanic_training.csv", sep = ";")
tt <- as.tibble(tt)
tt <- as_tibble(tt)
tt
tt <- read.csv(file = "Titanic_training.csv", sep = ";")
tt <- as_tibble(tt)
tt
tt$Embarked <- fct_recode(tt$Embarked, "Cherbourg" = "C", "Queenstown" = "Q", "Southampton" = "S")
tt
# Also, let's make Fare into a numeric variable
class(tt$Fare)
tt$Fare <- as.numeric(tt$Fare)
class(tt$Fare)
tt
# Have a look at the dataset
glimpse(tt)
summary(tt)
tt <- read.csv(file = "Titanic_training.csv", sep = ";", na.strings = "")
tt <- as_tibble(tt)
# It contains data on survived (Survived = 1) and deceased (Survived = 0) passengers, and their characteristics (i.e. ID, name, passenger class, sex, age, number of siblings and spouses on board, number of parents and children on board, the price of their fare, and the port where they embarked)
tt
# Let's rename the intransparent letters of the port where they embarked
tt$Embarked <- fct_recode(tt$Embarked, "Cherbourg" = "C", "Queenstown" = "Q", "Southampton" = "S")
tt
# Also, let's make Fare into a numeric variable
class(tt$Fare)
tt$Fare <- as.numeric(tt$Fare)
# Have a first look at the dataset
glimpse(tt)
summary(tt)
# And let's make Survived and Class into factors
class(tt$Survived)
class(tt$Class)
tt$Survived <- as.factor(tt$Survived)
tt$Class <- as.factor(tt$Class)
# Have a first look at the dataset
glimpse(tt)
summary(tt)
# Better now!
glimpse(tt)
summary(tt)
tt
tt$Embarked
# Have a first look at the dataset
tt
tt$Class
# Check order of factor levels
levels(tt$Embarked)
levels(tt$Class)
levels(tt$Survived) # all okay for now
# Better!
tt
summary(tt)
#
plot(tt)
str(tt)
summary(tt$SiblingsSpouses)
styler:::style_selection()
# same basic functions
min(tt$Fare, na.rm = TRUE)
max(tt$Fare, na.rm = T)
mean(tt$Fare, na.rm = T)
median(tt$Fare, na.rm = T)
hist(tt$Fare)
qqplot(tt$Fare)
qqnorm(tt$Fare)
hist(rnorm(200000, mean=78, sd=5)); norm <- rnorm(2000, mean=78, sd=20)
# same basic functions - numeric variable
table(tt$Survived)
addmargins(table(tt$Survived), FUN=sum)
addmargins(table(tt$Survived), FUN=mean)
# add an independent variable (categorical)
tapply(tt$Fare, tt$Class, median)
# add an independent variable (categorical)
tapply(tt$Fare, tt$Class, median, na.rm = T)
# add an independent variable (categorical)
tapply(tt$Fare, tt$Class, mean, na.rm = T)
# empty workspace
rm(list=ls(all=TRUE))
tt <- read.csv(file = "Titanic_training.csv", sep = ",", na.strings = "")
tt <- as_tibble(tt)
tt
tt <- read.csv(file = "Titanic_training.csv", sep = ",", na.strings = "")
tt <- as_tibble(tt)
# It contains data on survived (Survived = 1) and deceased (Survived = 0) passengers, and their characteristics (i.e. ID, name, passenger class, sex, age, number of siblings and spouses on board, number of parents and children on board, the price of their fare, and the port where they embarked)
tt
# Let's rename the intransparent letters of the port where they embarked
tt$Embarked <- fct_recode(tt$Embarked, "Cherbourg" = "C", "Queenstown" = "Q", "Southampton" = "S")
tt
# Oops, we forgot to make Survived and Class into factors
class(tt$Survived)
class(tt$Class)
tt$Survived <- as.factor(tt$Survived)
tt$Class <- as.factor(tt$Class)
# Check order of factor levels
levels(tt$Embarked)
levels(tt$Class)
levels(tt$Survived) # all okay for now
# Better!
tt
# overview per variable
summary(tt$SiblingsSpouses)
# overview per variable
summary(tt$SiblingsSpouses)
# same basic functions - numeric variable
min(tt$Fare, na.rm = TRUE)
max(tt$Fare, na.rm = T)
mean(tt$Fare, na.rm = T)
median(tt$Fare, na.rm = T)
# add an independent variable (categorical)
tapply(tt$Fare, tt$Class, median, na.rm = T)
# add an independent variable (categorical)
median(tt$Fare[tt$Class == "1"], na.rm = T)
# easier with tapply
tapply(tt$Fare, tt$Class, median, na.rm = T) # apply function 'median' to 'Fare', split up by 'Class'
# same basic functions - numeric variable
min(tt$Fare, na.rm = TRUE)
max(tt$Fare, na.rm = T)
mean(tt$Fare, na.rm = T)
median(tt$Fare, na.rm = T)
# add an independent variable (categorical)
median(tt$Fare[tt$Class == "1"], na.rm = T)
median(tt$Fare[tt$Class == "2"], na.rm = T)
median(tt$Fare[tt$Class == "3"], na.rm = T)
# easier with tapply
tapply(tt$Fare, tt$Class, median, na.rm = T) # apply function 'median' to 'Fare', split up by 'Class'
# frequency table
table(tt$Survived)
# frequency table
table(tt$Survived)
# totals?
addmargins(table(tt$Survived), FUN=sum) # try also other functions, e.g. mean
# relative frequency
prop.table(table(tt$Survived)) # table with relative frequency counts (weighed over 1)
# percentage?
prop.table(table(tt$Survived))*100 # over 100
# round
round(prop.table(table(tt$Survived))*100, 2) # two decimals
# different way, same result:
surv_table <- table(tt$Survived)
surv_prop <- prop.table(surv_table)*100
round(surv_prop, 2)
# save
write.table(surv_prop, "surv_prop.txt", sep="\t", quote=FALSE, row.names=F)
write_xlsx(x = surv_prop, path = "surv_prop.txt")
as.data.frame(surv_prop)
write_xlsx(x = as.data.frame(surv_prop), path = "surv_prop.txt")
# save
write.table(surv_prop, "surv_prop.txt", sep="\t", quote=FALSE, row.names=F)
write_xlsx(x = as.data.frame(surv_prop), path = "surv_prop.xlsx")
library(knitr)
### 6.3 Visualization - base package -----
kable(surv_prop)
kable_styling(surv_prop)
library(kableExtra)
install.packages("kableExtra")
library(kableExtra)
kable(surv_prop)
kable_as_image(surv_prop)
kable_classic(surv_prop)
kable_styling(surv_prop)
kable(surv_prop)
library(knitr)
kable(surv_prop)
detach("package:kableExtra", unload = TRUE)
library(knitr)
kable(surv_prop)
#### --- | exercise: terrorism in newspapers  ---#####
# We want to know if newspapers nowadays write more often about terrorism than in the past. We will load part of the Dutch-language VU-DNC newspaper corpus, which contains the archives of the NRC newspaper from 1950 and 2002. We'll search that for the words 'terrorisme', 'terreur', 'terrorist' and 'terroristen', and we'll assume that the use of these words is a proxy for dealing with the topic of terrorism.
# 1. Use a loop to load all corpus files from the "nrc1950" and "nrc2002" folders. Load them as separate subcorpora (`corpus.1950` and `corpus.2002`, i.e. two separate loops), and load in each line separately (line by line).
# 2. Compare the number of files for each subcorpus, and use the wordcount() function of the (to be installed) 'ngram'-package to count the number of words in each subcorpus.
# 3. Use 'exact matches' to search for the mentioned words in both subcorpora.
# 4. Compare the number of results per subcorpus, and weigh them over the total number of words in each subcorpus. Can we find an answer to our research question?
### 6. DATA EXPLORATION AND VISUALIZATION -----
### 6.1 Preparation -----
# empty workspace
rm(list=ls(all=TRUE))
# For this section, we'll work with a slightly modified version of the well-known Titanic dataset (source: 'Titanic: Machine Learning from Disaster' - https://www.kaggle.com/c/titanic/), which is often used in machine learning.
tt <- read.csv(file = "Titanic_training.csv", sep = ",", na.strings = "")
tt <- as_tibble(tt)
# It contains data on survived (Survived = 1) and deceased (Survived = 0) passengers, and their characteristics (i.e. ID, name, passenger class, sex, age, number of siblings and spouses on board, number of parents and children on board, the price of their fare, and the port where they embarked)
tt
# Let's rename the intransparent letters of the port where they embarked
tt$Embarked <- fct_recode(tt$Embarked, "Cherbourg" = "C", "Queenstown" = "Q", "Southampton" = "S")
tt
# Have a first look at the dataset
tt
str(tt)
summary(tt)
# Oops, we forgot to make Survived and Class into factors
class(tt$Survived)
class(tt$Class)
tt$Survived <- as.factor(tt$Survived)
tt$Class <- as.factor(tt$Class)
# Check order of factor levels
levels(tt$Embarked)
levels(tt$Class)
levels(tt$Survived) # all okay for now
# Better!
tt
str(tt)
summary(tt)
# A first visual view
plot(tt)
### 6.2 Numerically summarizing - base package -----
# overview per variable
summary(tt$SiblingsSpouses)
# NUMERIC VARIABLES
# same basic functions
min(tt$Fare, na.rm = TRUE)
max(tt$Fare, na.rm = T)
mean(tt$Fare, na.rm = T)
median(tt$Fare, na.rm = T)
# add an independent variable (categorical)
median(tt$Fare[tt$Class == "1"], na.rm = T)
median(tt$Fare[tt$Class == "2"], na.rm = T)
median(tt$Fare[tt$Class == "3"], na.rm = T)
# easier with tapply
tapply(tt$Fare, tt$Class, median, na.rm = T) # apply function 'median' to 'Fare', split up by 'Class'
# CATEGORICAL VARIABLES
# same basic functions
# frequency table
table(tt$Survived)
# totals?
addmargins(table(tt$Survived), FUN=sum) # try also other functions, e.g. mean
# relative frequency
prop.table(table(tt$Survived)) # table with relative frequency counts (weighed over 1)
# percentage?
prop.table(table(tt$Survived))*100 # over 100
# round
round(prop.table(table(tt$Survived))*100, 2) # two decimals
# different way, same result:
surv_table <- table(tt$Survived)
surv_prop <- prop.table(surv_table)*100
round(surv_prop, 2)
# save
write.table(surv_prop, "surv_prop.txt", sep="\t", quote=FALSE, row.names=F)
write_xlsx(x = as.data.frame(surv_prop), path = "surv_prop.xlsx")
### 6.3 Visualization - base package -----
# visualizing one numeric variable - histograms
hist(tt$Fare)
qqnorm(tt$Fare)
kable(surv_prop)
table(tt$Age)
summary(tt$Age)
cut(tt$Age)
cut(tt$Age, breaks = 4)
cut(tt$Age, breaks = 4, labels = c("adolescent", "young", "middle-aged", "elderly"))
summary(tt$Age)
cut(tt$Age, breaks = c(0, 18, 30, 50, 100)
)
cut(tt$Age, breaks = c(0, 18, 30, 50, 100), labels = c("adolescent", "young", "middle-aged", "elderly"))
# save/assign
tt$Age_fct <- cut(tt$Age, breaks = c(0, 18, 30, 50, 100), labels = c("adolescent", "young", "middle-aged", "elderly"))
tt
# save/assign
tt$Age_fct <- cut(tt$Age, breaks = c(0, 18, 30, 50, 100), labels = c("adolescent", "young", "middle-aged", "elderly"))
tt
prop.table(table(tt$Age_fct))*100 # over 100
tapply(tt$Fare, tt$Age_fct, median, na.rm = T) # apply function 'median' to 'Fare', split up by 'Class'
# table with absolute freqs
table(tt$Survived, tt$Class) # 2 dimensions
# table with absolute freqs
table(tt$Survived, tt$Sex) # 2 dimensions
# row and column totals
addmargins(table(tt$Survived, tt$Sex))
# table with relative freqs
prop.table(table(tt$Survived, tt$Sex), 2) # each column totals 100 (= read: relative distribution of DV for each level of IV)
# round?
round(prop.table(table(tt$Survived, tt$Sex), 2), 2)
# percentages?
round(prop.table(table(tt$Survived, tt$Sex), 2), 2)*100
# percentages?
round(prop.table(table(tt$Survived, tt$Sex), 2), 3)*100
round(prop.table(table(tt$Survived, tt$Age), 2), 3)*100
# problem:
round(prop.table(table(tt$Survived, tt$Age_fct), 2), 3)*100
# problem:
round(prop.table(table(tt$Survived, tt$Age), 2), 3)*100
summary(tt$Age)
# explore
round(prop.table(table(tt$Survived, tt$Age_fct), 2), 3)*100
# visualizing one numeric variable - histograms
hist(tt$Fare)
# add more independent variables?
ftable(tt$Age_fct, tt$Sex, tt$Survived)
round(prop.table(ftable(tt$Age_fct, tt$Sex, tt$Survived) , 1), 2)
# add more independent variables?
ftable(tt$Class, tt$Sex, tt$Survived)
round(prop.table(ftable(tt$Class, tt$Sex, tt$Survived) , 1), 2)
